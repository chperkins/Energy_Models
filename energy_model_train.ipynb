{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "energy_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NGGnXllPAVT",
        "outputId": "2ee8f36d-cb72-4067-81e9-43258d79d132"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/ME')\n",
        "sys.path.append('/ME/My Drive/energy/')\n",
        "\n",
        "import torch as t\n",
        "import torchvision.transforms as tr\n",
        "import torchvision.datasets as datasets\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import download_flowers_data, plot_ims, plot_diagnostics\n",
        "import math\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "#next steps - plot gradient if previous loss is positive, plot if previous loss is negative\n",
        "#check size of norm progression for coopnets\n",
        "#convert to TPU\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /ME; to attempt to forcibly remount, call drive.mount(\"/ME\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llfoFOruWz0e"
      },
      "source": [
        "config = {\n",
        "  \"seed\": 123, #for replicability\n",
        "  \"z_size\": 100, #number of latent variables in latent z for coopnets\n",
        "  'img_size': 32, #width/length of image\n",
        "  'num_channels': 1, #number of channels of image, 1 or 3\n",
        "  'langevin_step_num_gen': 0, #number of langevin/sgd steps for generator, usually 0\n",
        "  'batch_size': 100, #number of samples per iteration\n",
        "  'lr_gen': 0.0001, #learning rate of generator\n",
        "  'beta1_gen': 0.5, #beta1 for optimizer of generator\n",
        "  'sigma_gen': 0.3, #sigma for langevin dynamics of generator\n",
        "  'lr_des': 0.0001, #learning rate of descriptor\n",
        "  'beta1_des': 0.9, #beta1 for optimizer of descriptor\n",
        "  'with_noise': 'no', #use sgd with/without noise for descriptor\n",
        "  'langevin_step_num_des': 100, #number of minimizing steps from initial points for descriptor  \n",
        "  'num_iter':  10000, #number of total training iterations\n",
        "  'init': 'uniform', #initialize samples - uniform, gaussian, generator, persistent\n",
        "  'langevin_step_size_des': 1.123, #step size for sgd/adam/langevin of descriptor\n",
        "  'noise_divide': 1, #for strict langevin, divide variance of noise\n",
        "  'data_epsilon': 0.03, #add noise to observed samples\n",
        "  'l2_energy_reg': 0, #penalty on energy in loss\n",
        "  'weight_decay': 0, #weight decay of optimizer\n",
        "  'minimizer': 'sgd', #minimzier of samples for descriptor - adam or sgd\n",
        "  'sgd_beta': 0, #momentum for sgd of synthesized samples\n",
        "  'test_size': 10000, #test size output of finished models for FID scores\n",
        "  'data': 'FashionMNIST', #dataset - usually mnist or cifar10\n",
        "  'spec_norm': 'no', #add spectral normialization\n",
        "  'n_f' : 64, #complexity of net\n",
        "  'leak' : 0.2, #leaky_relu parameter\n",
        "  'lr_decrease': 'no' #manually decrease learning rate\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLa_Q3wYWD7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b43a36-d187-40b4-b171-0e88cd883931"
      },
      "source": [
        "ebm_dir='/ME/My Drive/energy/'\n",
        "\n",
        "device = t.device('cuda' if t.cuda.is_available() else 'cpu') #use gpu\n",
        "\n",
        "#these are optimum beta for different initializations\n",
        "if config['init']=='uniform':\n",
        "  config['beta1_des'] = 0.9\n",
        "elif config['init']=='generator':\n",
        "  config['beta1_des'] = 0.5\n",
        "\n",
        "digit = None #if not none, select digit for unconditional modeling. if none, all 10 digits are modeled\n",
        "\n",
        "# directory for experiment result\n",
        "EXP_DIR = f\"{ebm_dir}out_data/{str(config['data'])}_{config['init']}_{config['langevin_step_size_des']}_step_{config['data_epsilon']}_noise_{config['lr_des']}_lr_{config['langevin_step_num_des']}_{config['minimizer']}_beta_{config['sgd_beta']}_{config['spec_norm']}_energy_{config['l2_energy_reg']}_{config['with_noise']}/\"\n",
        "print(EXP_DIR)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/ME/My Drive/energy/out_data/FashionMNIST_uniform_1.123_step_0.03_noise_0.0001_lr_100_sgd_beta_0_no_energy_0_no/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvXoClwXPcUt",
        "outputId": "db58b176-efd1-4289-b90a-93fe9f991f81"
      },
      "source": [
        "# make directory for saving results\n",
        "#WARNING: currently will delete current directory. Be careful here.\n",
        "if os.path.exists(EXP_DIR):\n",
        "  print('file already exists. overwrite')\n",
        "  shutil.rmtree(EXP_DIR)           # Removes all the subdirectories!\n",
        "  os.makedirs(EXP_DIR)\n",
        "  for folder in ['checkpoints', 'shortrun', 'longrun']:\n",
        "    os.mkdir(EXP_DIR + folder)\n",
        "else:\n",
        "  os.makedirs(EXP_DIR)\n",
        "  for folder in ['checkpoints', 'shortrun', 'longrun']:\n",
        "    os.mkdir(EXP_DIR + folder)\n",
        "\n",
        "#record configuration for easier reference from past experiments\n",
        "config_json = json.dumps(config)\n",
        "f = open(EXP_DIR+'config.json',\"w\")\n",
        "f.write(config_json)\n",
        "f.close()\n",
        "\n",
        "# set seed for cpu and CUDA, get device\n",
        "t.manual_seed(config['seed'])\n",
        "if t.cuda.is_available():\n",
        "  t.cuda.manual_seed_all(config['seed'])\n",
        "\n",
        "print(t.cuda.is_available())\n",
        "print(device) #confirm cuda"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file already exists. overwrite\n",
            "True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldokCzEkPhQ9",
        "outputId": "6ab65cce-e6eb-480a-de80-bbf1aff45662"
      },
      "source": [
        "print('Processing data...')\n",
        "# make tensor of training data\n",
        "#default values are range [-1, 1]\n",
        "#save_image puts this into scale of [0,1]\n",
        "#code largely borrowed from ebm-anatomy\n",
        "if config['data'] == 'flowers':\n",
        "  download_flowers_data()\n",
        "data = {'cifar10': lambda path, func: datasets.CIFAR10(root=path, transform=func, download=True),\n",
        "        'mnist': lambda path, func: datasets.MNIST(root=path, transform=func, download=True),\n",
        "        'flowers': lambda path, func: datasets.ImageFolder(root=path, transform=func),\n",
        "        'FashionMNIST': lambda path, func: datasets.FashionMNIST(root=path, transform=func, download=True)}\n",
        "\n",
        "transform = tr.Compose([tr.Resize(config['img_size']),\n",
        "                        tr.CenterCrop(config['img_size']),\n",
        "                        tr.ToTensor(),\n",
        "                        tr.Normalize(tuple(0.5*t.ones(config['num_channels'])), tuple(0.5*t.ones(config['num_channels'])))])\n",
        "q = t.stack([x[0] for x in data[config['data']]('./data/' + config['data'], transform)])\n",
        "labels = [x[1] for x in data[config['data']]('./data/' + config['data'], transform)]\n",
        "\n",
        "train_q = q[0:40000]\n",
        "test_q = q[40000:]\n",
        "train_labels = labels[0:40000]\n",
        "test_labels = labels[40000:]\n",
        "print(test_q.shape)\n",
        "#split to train/test - test not currently needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_PWbAchP5e8"
      },
      "source": [
        "#restrict to digit if option chosen\n",
        "#if so, we do single class generation\n",
        "#otherwise, multiclass\n",
        "if digit is not None:\n",
        "  #unconditional filtering\n",
        "  index_list = [i for i,val in enumerate(train_labels) if val==digit]\n",
        "  index_test = [i for i,val in enumerate(test_labels) if val==digit]\n",
        "  train_q = train_q[index_list,:,:,:].to(device)\n",
        "  test_q = test_q[index_test,:,:,:].to(device)\n",
        "\n",
        "if config['init'] == 'persistent':\n",
        "  s_t_0 = (2*(t.rand([500000, config['num_channels'], config['img_size'], config['img_size']])) - 1) #initialize uniformly random images for persistent CD\n",
        "#size of bank of images will change training from data -> uniform\n",
        "\n",
        "#test data is not currently used but is separated in case of future use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3CvkY3pQDb0"
      },
      "source": [
        "# Define Descriptor\n",
        "#1 input channel for mnist\n",
        "class Descriptor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Descriptor, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(config['num_channels'], config['n_f'], kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(config['n_f'], config['n_f'] * 2, kernel_size=4, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv2d(config['n_f'] * 2, config['n_f'] * 4, kernel_size=4, stride=2, padding=1)\n",
        "    self.conv4 = nn.Conv2d(config['n_f'] * 4, config['n_f'] * 8, kernel_size=4, stride=2, padding=1)\n",
        "    self.conv5 = nn.Conv2d(config['n_f'] * 8, 1, kernel_size=4, stride=1, padding=0)\n",
        "\n",
        "    #self.fc = nn.Linear(16384,1)\n",
        "    self.leakyrelu = nn.LeakyReLU(config['leak'])\n",
        "\n",
        "    #spectral norm option. For this, usually keep step size/num steps large\n",
        "    if config['spec_norm'] == 'spec_norm':\n",
        "      self.conv1 = nn.utils.spectral_norm(self.conv1)\n",
        "      self.conv2 = nn.utils.spectral_norm(self.conv2)\n",
        "      self.conv3 = nn.utils.spectral_norm(self.conv3)\n",
        "      self.conv4 = nn.utils.spectral_norm(self.conv4)\n",
        "      self.conv5 = nn.utils.spectral_norm(self.conv5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.conv3(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.conv4(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.conv5(out)\n",
        "    return out.squeeze()\n",
        "\n",
        "# define generator\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.convt1 = nn.ConvTranspose2d(config['z_size'], 256, kernel_size=4, stride=1, padding=0)\n",
        "    self.convt2 = nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
        "    self.convt3 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
        "    self.convt4 = nn.ConvTranspose2d(64, config['num_channels'], kernel_size=5, stride=2, padding=2, output_padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(256)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.leakyrelu = nn.LeakyReLU()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, z):\n",
        "    self.z = z\n",
        "    out = self.convt1(z)\n",
        "    out = self.bn1(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.convt2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.convt3(out)\n",
        "    out = self.bn3(out)\n",
        "    out = self.leakyrelu(out)\n",
        "    out = self.convt4(out)\n",
        "    out = self.tanh(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdN0RaLpQTet"
      },
      "source": [
        "# take sample data using ebm-anatomy method\n",
        "\n",
        "################################\n",
        "# ## FUNCTIONS FOR SAMPLING ## #\n",
        "################################\n",
        "\n",
        "# sample batch from given array of images\n",
        "def sample_image_set(image_set, num_samps):\n",
        "  rand_inds = t.randperm(image_set.shape[0])[0:num_samps]\n",
        "  return image_set[rand_inds], rand_inds\n",
        "\n",
        "# sample positive images from dataset distribution q (add noise to ensure min sd is at least langevin noise sd)\n",
        "def sample_q(num_samps):\n",
        "  x_q = sample_image_set(train_q, num_samps)[0]\n",
        "  return x_q\n",
        "\n",
        "def save_data_plot(data_in, dir_in, name_in):\n",
        "  #save after 200 iterations in case initial values are extremely large\n",
        "  fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
        "  ax.plot(data_in[200:])\n",
        "  fig.savefig(dir_in+'post_200_'+name_in+'.png')   # save the figure to file\n",
        "  plt.close(fig)    # close the figure window\n",
        "\n",
        "  fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
        "  ax.plot(data_in)\n",
        "  fig.savefig(dir_in+'all_'+name_in+'.png')   # save the figure to file\n",
        "  plt.close(fig)    # close the figure window\n",
        "\n",
        "  np.save(dir_in+name_in+'.npy', data_in) #save array for future use\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7JvP6eIQgKR"
      },
      "source": [
        "#Main CoopNets Class\n",
        "\n",
        "#G0/1/2, D0/1/2 are notation from coopnets paper\n",
        "class CoopNets(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CoopNets, self).__init__()\n",
        "    self.descriptor = Descriptor().cuda()\n",
        "    self.generator = Generator().cuda()\n",
        "\n",
        "  #Define Langevin Dynamics for Generator - largely untouched from coopnets code\n",
        "  def langevin_dynamics_generator(self, z, obs):\n",
        "    obs = obs.detach()\n",
        "    criterian = nn.MSELoss(size_average=False, reduce=True)\n",
        "    for i in range(config['langevin_step_num_gen']):\n",
        "      noise = Variable(t.randn(config['batch_size'], config['z_size'], 1, 1).cuda())\n",
        "      z = Variable(z, requires_grad=True)\n",
        "      gen_res = self.generator(z)\n",
        "      gen_loss = 1.0 / (2.0 * config['sigma_gen'] * config['sigma_gen']) * criterian(gen_res, obs)\n",
        "      gen_loss.backward()\n",
        "      grad = z.grad\n",
        "      z = z - 0.5 * config['langevin_step_size_gen'] * config['langevin_step_size_gen'] * (z + grad)\n",
        "      if config['with_noise'] == 'yes':\n",
        "        z += config['langevin_step_size_gen'] * noise\n",
        "\n",
        "      return z\n",
        "\n",
        "  #udpated langevin dynamics function for descriptor (main energy model)\n",
        "  #this is no longer actual langevin dynamics. Instead, it is ADAM or SGD\n",
        "  #with/without added noise\n",
        "  def langevin_dynamics_descriptor(self, x):\n",
        "      \n",
        "    x = Variable(x.data, requires_grad = True)\n",
        "    g_start = t.zeros(1).to(device) #save initial gradient\n",
        "    g_end = t.zeros(1).to(device) #save end gradient\n",
        "\n",
        "    #ADAM parameters\n",
        "    alpha = config['langevin_step_size_des']\n",
        "    beta_1 = 0.5\n",
        "    beta_2 = 0.999\t\t\t\t\t\t#initialize the values of the parameters\n",
        "    epsilon = 1e-8\n",
        "    m_t = 0 \n",
        "    v_t = 0 \n",
        "\n",
        "    for i in range(config['langevin_step_num_des']):\n",
        "      x_feature = self.descriptor(x) #return energy of x\n",
        "      grad = t.autograd.grad(x_feature.sum(), [x])[0] #capture gradient\n",
        "\n",
        "      #using code https://github.com/sagarvegad/Adam-optimizer/blob/master/Adam.py\n",
        "      if config['minimizer'] == 'adam': \n",
        "        g_t = grad\t\t#computes the gradient of the stochastic function\n",
        "        m_t = beta_1*m_t + (1-beta_1)*g_t\t#updates the moving averages of the gradient\n",
        "        v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)\t#updates the moving averages of the squared gradient\n",
        "        m_cap = m_t/(1-(beta_1**(i+1)))\t\t#calculates the bias-corrected estimates\n",
        "        v_cap = v_t/(1-(beta_2**(i+1)))\t\t#calculates the bias-corrected estimates\n",
        "        x_prev = x\t\t\t\t\t\t\t\t\n",
        "        x = x_prev + (alpha*m_cap)/(t.sqrt(v_cap)+epsilon)\t#updates the parameters\n",
        "\n",
        "      #alternatively run SGD with/without momementum/noise\n",
        "      if config['minimizer'] == 'sgd':\n",
        "        if (i==0):\n",
        "          sgd_m_t = grad\n",
        "        else:\n",
        "          sgd_m_t = config['sgd_beta']*sgd_m_t + (1-config['sgd_beta'])*grad\n",
        "        x = x + config['langevin_step_size_des']*sgd_m_t #add gradient\n",
        "        if config['with_noise']=='yes':\n",
        "          x = x + 1e-2 * t.randn_like(x) #add noise to maximization\n",
        "\n",
        "      if (i==0):\n",
        "        g_start = grad.view(grad.shape[0], -1).norm(dim=1).mean() #store start gradient\n",
        "\n",
        "    g_end = grad.view(grad.shape[0], -1).norm(dim=1).mean() #store end gradient\n",
        "    return x, g_start, g_end\n",
        "\n",
        "  #Main training function\n",
        "  def train(self):\n",
        "\n",
        "    #initialize optimizers\n",
        "    des_optimizer = t.optim.Adam(self.descriptor.parameters(), lr=config['lr_des'], weight_decay = config['weight_decay'],\n",
        "                                         betas=[config['beta1_des'], 0.999])\n",
        "    gen_optimizer = t.optim.Adam(self.generator.parameters(), lr=config['lr_gen'],\n",
        "                                         betas=[config['beta1_gen'], 0.999])\n",
        "    mse_loss = t.nn.MSELoss(size_average=False, reduce=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    #store diagnostics as empty\n",
        "    gen_loss_epoch, des_loss_epoch, recon_loss_epoch = [], [], [] #losses\n",
        "    energy_data = [] #energy of data\n",
        "    energy_start, energy_end = [], [] #energy at start and end of GD \n",
        "    gradient_start, gradient_end = [], [] #gradient at start and end of GD\n",
        "\n",
        "    #train for 'num_iter'\n",
        "    for i in range(config['num_iter']):\n",
        "      \n",
        "      #sample observations and add noise\n",
        "      obs_data = sample_q(config['batch_size'])\n",
        "      random_noise = t.randn(config['batch_size'], config['num_channels'], config['img_size'], config['img_size'])\n",
        "      obs_data = obs_data + random_noise*config['data_epsilon']\n",
        "      obs_data = Variable(obs_data.cuda())\n",
        "\n",
        "      # G0\n",
        "      #sample from generator\n",
        "      if config['init'] == 'generator':\n",
        "        z = t.randn(config['batch_size'], config['z_size'], 1, 1)\n",
        "        z = Variable(z.cuda(), requires_grad=True)\n",
        "        gen_res = self.generator(z)\n",
        "\n",
        "      # other sampling methods\n",
        "      if config['init'] == 'gaussian':\n",
        "        gen_res = t.randn([config['batch_size'], config['num_channels'], config['img_size'], config['img_size']]).cuda()\n",
        "      if config['init'] == 'uniform':\n",
        "        gen_res = (2*(t.rand([config['batch_size'], config['num_channels'], config['img_size'], config['img_size']])) - 1).cuda()\n",
        "      if config['init'] == 'data':\n",
        "        gen_res = sample_q(config['batch_size'])\n",
        "      if config['init'] == 'persistent':\n",
        "        gen_res, rand_inds = sample_image_set(s_t_0, config['batch_size'])\n",
        "        random_noise = t.randn(config['batch_size'], config['num_channels'], config['img_size'], config['img_size'])\n",
        "        gen_res = (gen_res + random_noise*config['data_epsilon']).cuda()\n",
        "      gen_res_copy = gen_res.detach().clone()\n",
        "          \n",
        "      # D1\n",
        "      if config['langevin_step_num_des'] > 0:\n",
        "        revised, g_start, g_end = self.langevin_dynamics_descriptor(gen_res)\n",
        "      # G1\n",
        "      if config['init'] == 'generator':\n",
        "        if config['langevin_step_num_gen'] > 0:\n",
        "          z = self.langevin_dynamics_generator(z, revised)\n",
        "      # D2 - collect energies\n",
        "      obs_feature = self.descriptor(obs_data)\n",
        "      revised_feature = self.descriptor(revised)\n",
        "      \n",
        "      #compute description loss as difference in energies - ensures matching gradient from EBM theory\n",
        "      des_loss = (revised_feature.mean() - obs_feature.mean()).sum() \n",
        "      \n",
        "      #if values get too large, stop training\n",
        "      if abs(des_loss.detach().cpu().numpy()) > 1e+11:\n",
        "        print('error diverged ')\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'mcmc_{:>06d}_failiure.png'.format(i+1), revised)\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'init_{:>06d}_failure.png'.format(i+1), gen_res[0:config['batch_size']])\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'data_{:>06d}_failure.png'.format(i+1), obs_data)\n",
        "        break\n",
        "\n",
        "      #iterate training of descriptor model\n",
        "      des_optimizer.zero_grad()\n",
        "      des_loss.backward()\n",
        "      des_optimizer.step()\n",
        "\n",
        "      # G2\n",
        "      #iterate training of generator\n",
        "      ini_gen_res = gen_res.detach() # \n",
        "      if config['init'] == 'generator':\n",
        "        if config['langevin_step_num_gen'] > 0:\n",
        "          gen_res = self.generator(z)\n",
        "        gen_loss = 1.0 / (2.0 * config['sigma_gen'] * config['sigma_gen']) * mse_loss(gen_res,\n",
        "                                                                                      revised.detach())\n",
        "\n",
        "        gen_optimizer.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "        gen_loss_epoch.append(gen_loss.cpu().data)\n",
        "        recon_loss = mse_loss(revised, ini_gen_res)\n",
        "        recon_loss_epoch.append(recon_loss.cpu().data)\n",
        "\n",
        "      #store diagnostics\n",
        "      des_loss_epoch.append(des_loss.cpu().data)\n",
        "      energy_data.append(obs_feature.mean().cpu().data)\n",
        "      energy_end.append(revised_feature.mean().cpu().data)\n",
        "      energy_start.append(self.descriptor(gen_res_copy).mean().cpu().data)\n",
        "      gradient_start.append(g_start.cpu().data)\n",
        "      gradient_end.append(g_end.cpu().data)\n",
        "\n",
        "      #modify learning rate as model continues\n",
        "      if config['lr_decrease'] == 'yes':\n",
        "        for param_group in des_optimizer.param_groups:\n",
        "          param_group['lr'] = param_group['lr']*.999\n",
        "\n",
        "      #persistent update\n",
        "      if config['init'] == 'persistent':\n",
        "        # update persistent image bank\n",
        "        s_t_0[rand_inds] = revised.detach().cpu().clone()\n",
        "          \n",
        "      #store synthesized images as training runs\n",
        "      if (i + 1) == 1 or (i + 1) % 200 == 0:\n",
        "        # visualize synthesized images\n",
        "        if config['init'] == 'generator':\n",
        "          print('gen_loss')\n",
        "          print(gen_loss)\n",
        "        print('des_loss')\n",
        "        print(des_loss)\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'mcmc_{:>06d}.png'.format(i+1), revised)\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'init_{:>06d}.png'.format(i+1), gen_res[0:config['batch_size']])\n",
        "        plot_ims(EXP_DIR + 'shortrun/' + 'data_{:>06d}.png'.format(i+1), obs_data)\n",
        "\n",
        "      #what is outputted here often changes to test how the model makes results \n",
        "      #that are a different initialization than trained\n",
        "      #test different number of steps or different noise initialization\n",
        "      if (i + 1) == 1 or (i+1) % 1000 == 0 or i == 199 or i == 99 or i == 299 or i == 399:\n",
        "        long_init = (2*(t.rand([config['batch_size'], config['num_channels'], config['img_size'], config['img_size']])) - 1).cuda() #uniform\n",
        "        plot_ims(EXP_DIR + 'longrun/' + 'init_{:>06d}.png'.format(i+1), long_init)\n",
        "        for lgn in range(1):\n",
        "          long_init, trash1, trash2 = self.langevin_dynamics_descriptor(long_init)\n",
        "        plot_ims(EXP_DIR + 'longrun/' + 'mcmc_{:>06d}.png'.format(i+1), long_init)\n",
        "\n",
        "    #save network weights\n",
        "    t.save(self.descriptor, EXP_DIR + 'checkpoints/' + 'final_descriptor.pth')\n",
        "    t.save(self.generator, EXP_DIR + 'checkpoints/' + 'final_generator.pth')\n",
        "\n",
        "    #save diagnostic graphs\n",
        "    save_data_plot(des_loss_epoch, EXP_DIR, 'des_loss')\n",
        "    save_data_plot(gen_loss_epoch, EXP_DIR, 'gen_loss')\n",
        "    save_data_plot(recon_loss_epoch, EXP_DIR, 'recon_loss')\n",
        "    save_data_plot(energy_data, EXP_DIR, 'energy_data')\n",
        "    save_data_plot(energy_start, EXP_DIR, 'energy_start')\n",
        "    save_data_plot(energy_end, EXP_DIR, 'energy_end')\n",
        "    save_data_plot(gradient_start, EXP_DIR, 'gradient_start')\n",
        "    save_data_plot(gradient_end, EXP_DIR, 'gradient_end')\n",
        "\n",
        "  #output test images from completed model\n",
        "  def test(self, d_path, g_path, test_size, init_type):\n",
        "        \n",
        "    if init_type == 'generator':\n",
        "      self.generator = t.load(g_path).eval()\n",
        "\n",
        "    self.descriptor = t.load(d_path).eval()\n",
        "\n",
        "    #initialize data by init_type\n",
        "    if init_type == 'generator':\n",
        "      z = t.randn(test_size, config['z_size'], 1, 1)\n",
        "      z = Variable(z.cuda())\n",
        "      gen_res = self.generator(z)\n",
        "    if init_type == 'gaussian':\n",
        "      gen_res = t.randn([test_size, config['num_channels'], config['img_size'], config['img_size']]).cuda()\n",
        "    if init_type == 'uniform':\n",
        "      gen_res = (2*(t.rand([test_size, config['num_channels'], config['img_size'], config['img_size']])) - 1).cuda()\n",
        "    if init_type == 'persistent':\n",
        "      gen_res, rand_inds = sample_image_set(s_t_0, test_size)\n",
        "      gen_res = gen_res.cuda()\n",
        "\n",
        "    #run through energy minimization\n",
        "    revised, trash1, trash2 = self.langevin_dynamics_descriptor(gen_res)\n",
        "\n",
        "    #for i in range(test_size):\n",
        "    #  plot_ims(EXP_DIR + 'test_sample/' + 'init_{:>06d}.png'.format(i+1), revised[i,:,:,:])\n",
        "    #save numpy file for FID computation\n",
        "    np.save(EXP_DIR+'test_model.npy', revised.detach().cpu().numpy())\n",
        "\n",
        "    print ('===Image generation done.===')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dx-bhgrQjgv"
      },
      "source": [
        "model=CoopNets()\n",
        "model.train()\n",
        "model.test(EXP_DIR+'checkpoints/final_descriptor.pth',EXP_DIR+'checkpoints/final_generator.pth', config['test_size'], config['init'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}